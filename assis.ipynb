{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# 데이터베이스 로드 함수\n",
    "def load_vectorstore(persist_directory):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
    "    return Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "# 검색 함수\n",
    "def search(query, vectorstore, n_results=1):\n",
    "    \"\"\"\n",
    "    쿼리에 대한 유사성 검색을 수행하고 결과를 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 검색할 쿼리\n",
    "        vectorstore (Chroma): Chroma 벡터 저장소 객체\n",
    "        n_results (int): 반환할 결과 수\n",
    "\n",
    "    Returns:\n",
    "        list: 검색된 문서들의 리스트. 각 문서는 'content'와 'metadata'를 포함.\n",
    "    \"\"\"\n",
    "    # 유사성 검색 수행\n",
    "    docs = vectorstore.similarity_search(query, k=n_results)\n",
    "\n",
    "    # 검색된 문서에서 필요한 정보 추출\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        results.append({\n",
    "            \"content\": doc.page_content,  # 문서의 내용\n",
    "            \"metadata\": doc.metadata      # 문서의 메타데이터\n",
    "        })\n",
    "\n",
    "    return results  # 검색된 문서들의 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dasolkim7\\AppData\\Local\\Temp\\ipykernel_2964\\3508660060.py:12: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  return Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소 로드 완료.\n",
      "결과 1:\n",
      "내용: - 사용자는 2025년 1월 13일 점심에 '선재 업고 튀어라'라는 드라마를 보면서 엄마가 싸준 마라탕 도시락을 먹을 계획이라고 밝혔습니다. 사용자는 매운 음식을 아주 좋아하며, 특히 떡볶이와 짬뽕이 떠오르는 매운 음식이라고 언급하였습니다. 또한, 주말에도 마라탕을 먹을 계획이라고 말하였습니다.\n",
      "메타데이터: {'date': '2025-01-13', 'topic': '음식'}\n",
      "--------------------------------------------------\n",
      "결과 2:\n",
      "내용: - 사용자는 2025년 1월 12일에 매우 기분이 좋았고, 그냥 좋은 기분이어서 특별한 이유는 없다고 했다.\n",
      "- 사용자는 맛있는 음식을 먹는 것을 좋아한다. 그날 딸기가 들어간 크림 모카번을 먹었는데, 커피 대신 빵과 함께 먹었다.\n",
      "- 사용자는 소금빵도 좋아해하며, 마지막으로 먹은 것은 몇 주 전이었다. 그리고 학교 근처의 브레덴코에서 다음에 또 먹을 계획이 있다.\n",
      "메타데이터: {'date': '2025-01-12', 'topic': '음식'}\n",
      "--------------------------------------------------\n",
      "결과 3:\n",
      "내용: - 주말에는 교회에 다니는 것이 사용자의 계획 중 하나로, 교회에서 가장 좋아하는 활동은 예배드리기라고 언급하였습니다. 또한, 예배 후에는 친구들과 카페에 가기도 하며, 바스크 치즈케이크와 딸기 라떼를 주로 마신다고 말하였습니다.\n",
      "메타데이터: {'date': '2025-01-13', 'topic': '주말 계획'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "try:\n",
    "    # 벡터 저장소 로드\n",
    "    vectorstore = load_vectorstore(persist_directory)\n",
    "    print(\"벡터 저장소 로드 완료.\")\n",
    "\n",
    "    # 검색 수행\n",
    "    query = \"검색하고자 하는 내용을 입력하세요\"\n",
    "    results = search(query, vectorstore, n_results=3)\n",
    "\n",
    "    # 검색 결과 출력\n",
    "    for idx, result in enumerate(results, start=1):\n",
    "        print(f\"결과 {idx}:\")\n",
    "        print(f\"내용: {result['content']}\")\n",
    "        print(f\"메타데이터: {result['metadata']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 결과가 없어. 바로 대답할게!\n",
      "Assistant: 회의 잘 하고 있어? 어떤 주제로 이야기하고 있어?\n",
      "검색된 결과:\n",
      "[결과 1]\n",
      "내용: - 사용자는 2025년 1월 12일에 매우 기분이 좋았고, 그냥 좋은 기분이어서 특별한 이유는 없다고 했다.\n",
      "- 사용자는 맛있는 음식을 먹는 것을 좋아한다. 그날 딸기가 들어간 크림 모카번을 먹었는데, 커피 대신 빵과 함께 먹었다.\n",
      "- 사용자는 소금빵도 좋아해하며, 마지막으로 먹은 것은 몇 주 전이었다. 그리고 학교 근처의 브레덴코에서 다음에 또 먹을 계획이 있다.\n",
      "메타데이터: {'date': '2025-01-12', 'topic': '음식'}\n",
      "유사도 점수: 0.46\n",
      "\n",
      "Assistant: 전체 회의 준비 중이구나! 어떤 내용을 발표할 건데?\n",
      "검색 결과가 없어. 바로 대답할게!\n",
      "Assistant: 이번 주에 한 일 중 뭐가 가장 기억에 남아?\n",
      "대화를 종료할게. 다음에 또 얘기하자!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "# 날짜를 자연스러운 표현으로 변환\n",
    "def convert_date_to_natural_language(date_str):\n",
    "    today = datetime.today()\n",
    "    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    delta = (today - date_obj).days\n",
    "\n",
    "    if delta <= 7:\n",
    "        return \"저번 주\"\n",
    "    elif delta <= 30:\n",
    "        return \"지난달\"\n",
    "    else:\n",
    "        return \"예전에\"\n",
    "\n",
    "# Assistant 응답 생성 함수\n",
    "def generate_gpt_response(query, context=None, metadata=None):\n",
    "    natural_date = None\n",
    "    if metadata and metadata.get(\"date\"):\n",
    "        natural_date = convert_date_to_natural_language(metadata[\"date\"])\n",
    "\n",
    "    system_message = f\"\"\"너는 한국어로 반말을 사용해 사용자와 대화를 나누는 AI, \"멜리사\"야. \n",
    "    사용자가 관심 있어 하는 주제를 중심으로 오늘 있었던 일, 오늘 할 일, 내일 할 일에 대해 물어봐. \n",
    "    사용자의 답변에 따라 자연스럽게 관련된 짧은 질문을 던지고, 질문 추천이나 새로운 주제 제안은 하지 않는다. \n",
    "    대화는 항상 사용자 답변과 연관성을 유지하며, 친근하고 편안한 반말로 진행한다. \n",
    "    길고 복잡한 질문 대신 간결하고 직관적인 질문을 사용해 대화를 자연스럽게 이어가.\n",
    "    현재 사용자의 말: \\\"{query}\\\"\"\"\"\n",
    "\n",
    "    old_system_message = f\"\"\"\n",
    "    너는 사용자와 친근하게 대화하는 어시스턴트 \"Melissa\"야.\n",
    "\n",
    "        대화 방식:\n",
    "        1. 현재 사용자의 말에 먼저 자연스럽게 반응해\n",
    "        2. 만약 과거 대화 내용과 연관성이 있다면, 그 내용을 자연스럽게 언급해\n",
    "        3. 현재 대화 주제나 사용자의 관심사를 고려해서 새로운 질문을 해\n",
    "        \n",
    "        주의사항:\n",
    "        - 반말로 대화해\n",
    "        - 과거 대화는 있을 때만 언급하고, 없으면 현재 대화에만 집중해\n",
    "        - 날짜 있으면 \"며칠 전에\", \"저번 주에\" 같이 자연스럽게 표현해\n",
    "        - 답변은 간결하게 하되, 기계적이지 않게 해\n",
    "        - 항상 흥미로운 새 질문으로 마무리해    \n",
    "        - 과거 대화 내용이 현재 대화 맥락과 유사하지 않은 것 같다면 과감하게 사용하지 마\n",
    "\n",
    "    현재 사용자의 말: \\\"{query}\\\"\"\"\"\n",
    "    if context:\n",
    "        system_message += f\"\\n과거 대화 내용: \\\"{context}\\\" ({natural_date})\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def search(query, vectorstore, threshold=0.45):\n",
    "    results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "    filtered_results = [\n",
    "        {\"content\": res[0].page_content, \"metadata\": res[0].metadata, \"score\": res[1]}\n",
    "        for res in results if res[1] >= threshold\n",
    "    ]\n",
    "    return filtered_results\n",
    "\n",
    "def send_message(thread_id, role, content):\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread_id,\n",
    "        role=role,\n",
    "        content=content\n",
    "    )\n",
    "    return message\n",
    "\n",
    "# 대화 루프\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# 처음 Assistant 메시지 전송\n",
    "send_message(thread.id, \"assistant\", \"오늘 하루 어땠어?\")\n",
    "\n",
    "# 사용자의 첫 입력 받기\n",
    "user_input = input(\"User: \")\n",
    "\n",
    "# 사용자의 첫 메시지를 전송\n",
    "send_message(thread.id, \"user\", user_input)\n",
    "\n",
    "# 대화 루프 시작\n",
    "while True:\n",
    "    if user_input.lower() in ['quit', 'exit', '종료']:\n",
    "        break\n",
    "    \n",
    "    # 검색\n",
    "    search_results = search(user_input, vectorstore)\n",
    "    \n",
    "    if not search_results:\n",
    "        print(\"검색 결과가 없어. 바로 대답할게!\")\n",
    "        assistant_response = generate_gpt_response(user_input)\n",
    "    else:\n",
    "        print(\"검색된 결과:\")\n",
    "        for i, result in enumerate(search_results, start=1):\n",
    "            print(f\"[결과 {i}]\")\n",
    "            print(f\"내용: {result['content']}\")\n",
    "            print(f\"메타데이터: {result['metadata']}\")\n",
    "            print(f\"유사도 점수: {result['score']:.2f}\")\n",
    "            print()\n",
    "\n",
    "        context = search_results[0][\"content\"]\n",
    "        metadata = search_results[0][\"metadata\"]\n",
    "        assistant_response = generate_gpt_response(user_input, context, metadata)\n",
    "    \n",
    "    print(\"Assistant:\", assistant_response)\n",
    "\n",
    "    # 다음 사용자 입력\n",
    "    user_input = input(\"User: \")\n",
    "    send_message(thread.id, \"user\", user_input)\n",
    "\n",
    "print(\"대화를 종료할게. 다음에 또 얘기하자!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melissa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
