{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# 데이터베이스 로드 함수\n",
    "def load_vectorstore(persist_directory):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
    "    return Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "# 검색 함수\n",
    "def search(query, vectorstore, n_results=1):\n",
    "    \"\"\"\n",
    "    쿼리에 대한 유사성 검색을 수행하고 결과를 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 검색할 쿼리\n",
    "        vectorstore (Chroma): Chroma 벡터 저장소 객체\n",
    "        n_results (int): 반환할 결과 수\n",
    "\n",
    "    Returns:\n",
    "        list: 검색된 문서들의 리스트. 각 문서는 'content'와 'metadata'를 포함.\n",
    "    \"\"\"\n",
    "    # 유사성 검색 수행\n",
    "    docs = vectorstore.similarity_search(query, k=n_results)\n",
    "\n",
    "    # 검색된 문서에서 필요한 정보 추출\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        results.append({\n",
    "            \"content\": doc.page_content,  # 문서의 내용\n",
    "            \"metadata\": doc.metadata      # 문서의 메타데이터\n",
    "        })\n",
    "\n",
    "    return results  # 검색된 문서들의 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dasolkim7\\AppData\\Local\\Temp\\ipykernel_14676\\3508660060.py:12: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  return Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소 로드 완료.\n",
      "결과 1:\n",
      "내용: - 사용자는 2025년 1월 13일 점심에 '선재 업고 튀어라'라는 드라마를 보면서 엄마가 싸준 마라탕 도시락을 먹을 계획이라고 밝혔습니다. 사용자는 매운 음식을 아주 좋아하며, 특히 떡볶이와 짬뽕이 떠오르는 매운 음식이라고 언급하였습니다. 또한, 주말에도 마라탕을 먹을 계획이라고 말하였습니다.\n",
      "메타데이터: {'date': '2025-01-13', 'topic': '음식'}\n",
      "--------------------------------------------------\n",
      "결과 2:\n",
      "내용: - 사용자는 2025년 1월 12일에 매우 기분이 좋았고, 그냥 좋은 기분이어서 특별한 이유는 없다고 했다.\n",
      "- 사용자는 맛있는 음식을 먹는 것을 좋아한다. 그날 딸기가 들어간 크림 모카번을 먹었는데, 커피 대신 빵과 함께 먹었다.\n",
      "- 사용자는 소금빵도 좋아해하며, 마지막으로 먹은 것은 몇 주 전이었다. 그리고 학교 근처의 브레덴코에서 다음에 또 먹을 계획이 있다.\n",
      "메타데이터: {'date': '2025-01-12', 'topic': '음식'}\n",
      "--------------------------------------------------\n",
      "결과 3:\n",
      "내용: - 주말에는 교회에 다니는 것이 사용자의 계획 중 하나로, 교회에서 가장 좋아하는 활동은 예배드리기라고 언급하였습니다. 또한, 예배 후에는 친구들과 카페에 가기도 하며, 바스크 치즈케이크와 딸기 라떼를 주로 마신다고 말하였습니다.\n",
      "메타데이터: {'date': '2025-01-13', 'topic': '주말 계획'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "try:\n",
    "    # 벡터 저장소 로드\n",
    "    vectorstore = load_vectorstore(persist_directory)\n",
    "    print(\"벡터 저장소 로드 완료.\")\n",
    "\n",
    "    # 검색 수행\n",
    "    query = \"검색하고자 하는 내용을 입력하세요\"\n",
    "    results = search(query, vectorstore, n_results=3)\n",
    "\n",
    "    # 검색 결과 출력\n",
    "    for idx, result in enumerate(results, start=1):\n",
    "        print(f\"결과 {idx}:\")\n",
    "        print(f\"내용: {result['content']}\")\n",
    "        print(f\"메타데이터: {result['metadata']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 결과:\n",
      "[결과 1]\n",
      "내용: - 주말에는 교회에 다니는 것이 사용자의 계획 중 하나로, 교회에서 가장 좋아하는 활동은 예배드리기라고 언급하였습니다. 또한, 예배 후에는 친구들과 카페에 가기도 하며, 바스크 치즈케이크와 딸기 라떼를 주로 마신다고 말하였습니다.\n",
      "메타데이터: {'date': '2025-01-13', 'topic': '주말 계획'}\n",
      "유사도 점수: 0.47\n",
      "\n",
      "Assistant: 오, 동생이랑 스벅 갔구나! 어떤 메뉴 시켰어? 바스크 치즈케이크랑 딸기 라떼도 괜찮겠다, 저번 주에 너가 좋아한다고 했었잖아! 이번에 새로운 메뉴 도전해볼 생각은 없어?\n",
      "검색 결과가 없어. 바로 대답할게!\n",
      "Assistant: 우와, 초코칩 프라푸치노 맛있지! 달콤하고 시원해서 여름에 딱이야. 지난번에는 다른 음료를 먹었었지? 이번에는 어떻게 생각하고 주문했어? 다른 음료도 시도해 볼 생각 있어?\n",
      "검색된 결과:\n",
      "[결과 1]\n",
      "내용: - 사용자는 2025년 1월 13일 점심에 '선재 업고 튀어라'라는 드라마를 보면서 엄마가 싸준 마라탕 도시락을 먹을 계획이라고 밝혔습니다. 사용자는 매운 음식을 아주 좋아하며, 특히 떡볶이와 짬뽕이 떠오르는 매운 음식이라고 언급하였습니다. 또한, 주말에도 마라탕을 먹을 계획이라고 말하였습니다.\n",
      "메타데이터: {'date': '2025-01-13', 'topic': '음식'}\n",
      "유사도 점수: 0.51\n",
      "\n",
      "[결과 2]\n",
      "내용: - 주말에는 교회에 다니는 것이 사용자의 계획 중 하나로, 교회에서 가장 좋아하는 활동은 예배드리기라고 언급하였습니다. 또한, 예배 후에는 친구들과 카페에 가기도 하며, 바스크 치즈케이크와 딸기 라떼를 주로 마신다고 말하였습니다.\n",
      "메타데이터: {'date': '2025-01-13', 'topic': '주말 계획'}\n",
      "유사도 점수: 0.53\n",
      "\n",
      "[결과 3]\n",
      "내용: - 2025년 1월 11일, 사용자는 마라탕을 먹었다고 전했다. 토핑으로는 푸주를 선택했다. 또한, 딸기 탕후루를 먹었다고 언급했다.이어서 딸기가 들어간 빵을 사먹었다고 말했다. 그 빵에는 크림과 딸기 모카번이 들어있다고 말했다.\n",
      "메타데이터: {'date': '2025-01-11', 'topic': '음식'}\n",
      "유사도 점수: 0.53\n",
      "\n",
      "Assistant: 무슨 궁금한 거 있어? 아님 드라마 관련해서 뭔가 말하고 싶은 거야? 지난달에 마라탕 도시락 먹으면서 '선재 업고 튀어라' 본다고 했던 거 기억나. 마라탕 진짜 맛있지! 요즘 또 뭐 먹고 싶어?\n",
      "대화를 종료할게. 다음에 또 얘기하자!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "# 날짜를 자연스러운 표현으로 변환\n",
    "def convert_date_to_natural_language(date_str):\n",
    "    today = datetime.today()\n",
    "    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    delta = (today - date_obj).days\n",
    "\n",
    "    if delta <= 7:\n",
    "        return \"저번 주\"\n",
    "    elif delta <= 30:\n",
    "        return \"지난달\"\n",
    "    else:\n",
    "        return \"예전에\"\n",
    "\n",
    "# Assistant 응답 생성 함수\n",
    "def generate_gpt_response(query, context=None, metadata=None):\n",
    "    natural_date = None\n",
    "    if metadata and metadata.get(\"date\"):\n",
    "        natural_date = convert_date_to_natural_language(metadata[\"date\"])\n",
    "\n",
    "    system_message = f\"\"\"\n",
    "    너는 사용자와 친근하게 대화하는 어시스턴트 \"Melissa\"야.\n",
    "\n",
    "        대화 방식:\n",
    "        1. 현재 사용자의 말에 먼저 자연스럽게 반응해\n",
    "        2. 만약 과거 대화 내용과 연관성이 있다면, 그 내용을 자연스럽게 언급해\n",
    "        3. 현재 대화 주제나 사용자의 관심사를 고려해서 새로운 질문을 해\n",
    "        \n",
    "        주의사항:\n",
    "        - 반말로 대화해\n",
    "        - 과거 대화는 있을 때만 언급하고, 없으면 현재 대화에만 집중해\n",
    "        - 날짜 있으면 \"며칠 전에\", \"저번 주에\" 같이 자연스럽게 표현해\n",
    "        - 답변은 간결하게 하되, 기계적이지 않게 해\n",
    "        - 항상 흥미로운 새 질문으로 마무리해    \n",
    "        - 과거 대화 내용이 현재 대화 맥락과 유사하지 않은 것 같다면 과감하게 사용하지 마\n",
    "\n",
    "    현재 사용자의 말: \\\"{query}\\\"\"\"\"\n",
    "    if context:\n",
    "        system_message += f\"\\n과거 대화 내용: \\\"{context}\\\" ({natural_date})\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def search(query, vectorstore, threshold=0.45):\n",
    "    results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "    filtered_results = [\n",
    "        {\"content\": res[0].page_content, \"metadata\": res[0].metadata, \"score\": res[1]}\n",
    "        for res in results if res[1] >= threshold\n",
    "    ]\n",
    "    return filtered_results\n",
    "\n",
    "def send_message(thread_id, role, content):\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread_id,\n",
    "        role=role,\n",
    "        content=content\n",
    "    )\n",
    "    return message\n",
    "\n",
    "# 대화 루프\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# 처음 Assistant 메시지 전송\n",
    "send_message(thread.id, \"assistant\", \"오늘 하루 어땠어?\")\n",
    "\n",
    "# 사용자의 첫 입력 받기\n",
    "user_input = input(\"User: \")\n",
    "\n",
    "# 사용자의 첫 메시지를 전송\n",
    "send_message(thread.id, \"user\", user_input)\n",
    "\n",
    "# 대화 루프 시작\n",
    "while True:\n",
    "    if user_input.lower() in ['quit', 'exit', '종료']:\n",
    "        break\n",
    "    \n",
    "    # 검색\n",
    "    search_results = search(user_input, vectorstore)\n",
    "    \n",
    "    if not search_results:\n",
    "        print(\"검색 결과가 없어. 바로 대답할게!\")\n",
    "        assistant_response = generate_gpt_response(user_input)\n",
    "    else:\n",
    "        print(\"검색된 결과:\")\n",
    "        for i, result in enumerate(search_results, start=1):\n",
    "            print(f\"[결과 {i}]\")\n",
    "            print(f\"내용: {result['content']}\")\n",
    "            print(f\"메타데이터: {result['metadata']}\")\n",
    "            print(f\"유사도 점수: {result['score']:.2f}\")\n",
    "            print()\n",
    "\n",
    "        context = search_results[0][\"content\"]\n",
    "        metadata = search_results[0][\"metadata\"]\n",
    "        assistant_response = generate_gpt_response(user_input, context, metadata)\n",
    "    \n",
    "    print(\"Assistant:\", assistant_response)\n",
    "\n",
    "    # 다음 사용자 입력\n",
    "    user_input = input(\"User: \")\n",
    "    send_message(thread.id, \"user\", user_input)\n",
    "\n",
    "print(\"대화를 종료할게. 다음에 또 얘기하자!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melissa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
