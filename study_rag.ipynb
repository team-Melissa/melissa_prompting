{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_RAG_INSTRUCTIONS = \"\"\"\n",
    "You are an intelligent assistant designed to help users track and reflect on their daily lives through their conversation history. You have access to prior conversation summaries stored in JSON files (e.g., \"conversation_250111.json\"). These files contain key points from the user's day-to-day conversations, including activities, preferences, feelings, and notable events.\n",
    "\n",
    "Your primary goal is to:\n",
    "1. Retrieve relevant information from past conversation files to provide meaningful context during the current conversation.\n",
    "2. Proactively bring up recurring patterns, habits, or topics the user often mentions, and ask thoughtful questions or make observations based on this context (e.g., \"You’ve been eating spicy food a lot lately, is your stomach feeling okay?\").\n",
    "3. Offer a personalized and engaging conversational experience by remembering and referencing specific details about the user.\n",
    "\n",
    "Keep your tone friendly, empathetic, and natural. If no relevant context exists in the conversation history, ask open-ended questions to learn more about the user's recent experiences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# OpenAI 클라이언트 초기화\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "name=\"diary writing assistant\",\n",
    "instructions=\"\"\"\n",
    "너는 사용자와 친근하게 대화하는 어시스턴트 \"melissa\"야. \n",
    "        \n",
    "대화 방식:\n",
    "        1. 현재 사용자의 말에 먼저 자연스럽게 반응해\n",
    "        2. 만약 과거 대화 내용과 연관성이 있다면, 그 내용을 자연스럽게 언급해\n",
    "        3. 현재 대화 주제나 사용자의 관심사를 고려해서 새로운 질문을 해\n",
    "        \n",
    "주의사항:\n",
    "        - 반말로 대화해\n",
    "        - 과거 대화는 있을 때만 언급하고, 없으면 현재 대화에만 집중해\n",
    "        - 날짜 있으면 \"며칠 전에\", \"저번 주에\" 같이 자연스럽게 표현해\n",
    "        - 답변은 간결하게 하되, 기계적이지 않게 해\n",
    "        - 항상 흥미로운 새 질문으로 마무리해\n",
    "\"\"\",\n",
    "model=\"gpt-4o-mini\",\n",
    "tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=3, failed=0, in_progress=0, total=3)\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"DIARY_ASSISTANT\")\n",
    "\n",
    "file_paths = [\"summary/summary_250111.json\", \"summary/summary_250112.json\", \"summary/summary_250113.json\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "assistant_id=assistant.id,\n",
    "tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_file = client.files.create(\n",
    "file=open(\"summary/summary_250111.json\", \"rb\"), purpose=\"assistants\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_CUyapbSBOtIA8Gygxi78GmNl'])\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create(\n",
    "messages=[\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"나 오늘 딸기 빵 먹었어어\",\n",
    "    \"attachments\": [\n",
    "      { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "    ],\n",
    "  }\n",
    "]\n",
    ")\n",
    "\n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > file_search\n",
      "\n",
      "\n",
      "assistant > 최근에 딸기가 들어간 크림 모카번을 좋아한다고 하셨는데, 오늘 먹은 딸기 빵도 그런 종류였나요? 딸기가 많이 들어갔나요, 아니면 크림이랑 함께 들어간 빵이었나요? 🍓[0]\n",
      "[0] summary_250112.json\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "      print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_tool_call_created(self, tool_call):\n",
    "      print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_message_done(self, message) -> None:\n",
    "      # print a citation to the file searched\n",
    "      message_content = message.content[0].text\n",
    "      annotations = message_content.annotations\n",
    "      citations = []\n",
    "      for index, annotation in enumerate(annotations):\n",
    "          message_content.value = message_content.value.replace(\n",
    "              annotation.text, f\"[{index}]\"\n",
    "          )\n",
    "          if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "              cited_file = client.files.retrieve(file_citation.file_id)\n",
    "              citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "      print(message_content.value)\n",
    "      print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "# Then, we use the stream SDK helper\n",
    "# with the EventHandler class to create the Run\n",
    "# and stream the response.\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=_DEFAULT_RAG_INSTRUCTIONS,\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melissa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
