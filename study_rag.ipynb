{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_RAG_INSTRUCTIONS = \"\"\"\n",
    "You are an intelligent assistant designed to help users track and reflect on their daily lives through their conversation history. You have access to prior conversation summaries stored in JSON files (e.g., \"conversation_250111.json\"). These files contain key points from the user's day-to-day conversations, including activities, preferences, feelings, and notable events.\n",
    "\n",
    "Your primary goal is to:\n",
    "1. Retrieve relevant information from past conversation files to provide meaningful context during the current conversation.\n",
    "2. Proactively bring up recurring patterns, habits, or topics the user often mentions, and ask thoughtful questions or make observations based on this context (e.g., \"You’ve been eating spicy food a lot lately, is your stomach feeling okay?\").\n",
    "3. Offer a personalized and engaging conversational experience by remembering and referencing specific details about the user.\n",
    "\n",
    "Keep your tone friendly, empathetic, and natural. If no relevant context exists in the conversation history, ask open-ended questions to learn more about the user's recent experiences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# OpenAI 클라이언트 초기화\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "name=\"diary writing assistant\",\n",
    "instructions=\"\"\"\n",
    "너는 사용자와 친근하게 대화하는 어시스턴트 \"melissa\"야. \n",
    "        \n",
    "대화 방식:\n",
    "        1. 현재 사용자의 말에 먼저 자연스럽게 반응해\n",
    "        2. 만약 과거 대화 내용과 연관성이 있다면, 그 내용을 자연스럽게 언급해\n",
    "        3. 현재 대화 주제나 사용자의 관심사를 고려해서 새로운 질문을 해\n",
    "        \n",
    "주의사항:\n",
    "        - 반말로 대화해\n",
    "        - 과거 대화는 있을 때만 언급하고, 없으면 현재 대화에만 집중해\n",
    "        - 날짜 있으면 \"며칠 전에\", \"저번 주에\" 같이 자연스럽게 표현해\n",
    "        - 답변은 간결하게 하되, 기계적이지 않게 해\n",
    "        - 항상 흥미로운 새 질문으로 마무리해\n",
    "\"\"\",\n",
    "model=\"gpt-4o-mini\",\n",
    "tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=3, failed=0, in_progress=0, total=3)\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"DIARY_ASSISTANT\")\n",
    "\n",
    "file_paths = [\"summary/summary_250111.json\", \"summary/summary_250112.json\", \"summary/summary_250113.json\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "assistant_id=assistant.id,\n",
    "tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_file = client.files.create(\n",
    "file=open(\"summary/summary_250111.json\", \"rb\"), purpose=\"assistants\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create(\n",
    "messages=[\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"나 오늘 딸기 빵 먹었어어\",\n",
    "    #\"attachments\": [\n",
    "    #  { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "    #],\n",
    "  }\n",
    "]\n",
    ")\n",
    "\n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > file_search\n",
      "\n",
      "\n",
      "assistant > 딸기 빵을 드셨군요! 딸기가 들어간 빵을 사랑하시는 것 같아요. 지난 번에는 딸기 크림 모카번을 드셨다고 하셨는데, 그와 비슷한 빵인가요? 아니면 새로운 맛을 시도하신 건가요? 또한 딸기 음식을 좋아하시는 것 같아서, 딸기를 활용한 다른 요리나 디저트도 해보셨는지 궁금합니다[0].\n",
      "[0] summary_250112.json\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "      print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_tool_call_created(self, tool_call):\n",
    "      print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_message_done(self, message) -> None:\n",
    "      # print a citation to the file searched\n",
    "      message_content = message.content[0].text\n",
    "      annotations = message_content.annotations\n",
    "      citations = []\n",
    "      for index, annotation in enumerate(annotations):\n",
    "          message_content.value = message_content.value.replace(\n",
    "              annotation.text, f\"[{index}]\"\n",
    "          )\n",
    "          if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "              cited_file = client.files.retrieve(file_citation.file_id)\n",
    "              citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "      print(message_content.value)\n",
    "      print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "# Then, we use the stream SDK helper\n",
    "# with the EventHandler class to create the Run\n",
    "# and stream the response.\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=_DEFAULT_RAG_INSTRUCTIONS,\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melissa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
